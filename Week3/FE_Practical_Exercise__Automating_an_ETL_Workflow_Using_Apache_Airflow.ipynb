{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhV2k4fjtxUEPTEVewUyWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falawar7/AAI_634O/blob/main/Week3/FE_Practical_Exercise__Automating_an_ETL_Workflow_Using_Apache_Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Exercise: Automating an ETL Workflow Using Apache Airflow**\n"
      ],
      "metadata": {
        "id": "DeDh9GKT6B_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-Step Instructions**\n",
        "\n",
        "**Step 1: Set Up Apache Airflow**\n",
        "\n",
        "\n",
        "*   Follow the Apache Airflow Installation Guide to install Airflow.\n",
        "*   Once installed, start the Airflow scheduler and web server:\n",
        "    *   airflow scheduler airflow webserver- port 8080.\n",
        "    * Access the Airflow UI at http://localhost:8080.\n",
        "\n"
      ],
      "metadata": {
        "id": "vRMs0B4a62oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Up MongoDB**\n",
        "\n",
        " • Install MongoDB by following the MongoDB installation guide.\n",
        "\n",
        " • Start the MongoDB service locally or use MongoDB Atlas, a cloud-based service"
      ],
      "metadata": {
        "id": "0l5K3Bxz7UEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create the DAG File**\n",
        " • In the dags/ directory of your Airflow installation, create a DAG file, for example:\n",
        " etl pipeline mongodb.py.\n",
        " • Define the DAG with three tasks: extract data, transform data, and load data. You\n",
        " can use the following code to define the DAG:\n",
        "\n",
        " from airflow import DAG\n",
        "\n",
        " from airflow.operators.python_operator import PythonOperator\n",
        "\n",
        " from datetime import datetime\n",
        "\n",
        " import pandas as pd\n",
        "\n",
        " from pymongo import MongoClient\n",
        "\n",
        " # Define default arguments for the DAG\n",
        "\n",
        " default_args = {\n",
        "\n",
        " ’owner’: ’airflow’,\n",
        "\n",
        " ’start_date’: datetime(2023, 1, 1),\n",
        "\n",
        " ’retries’: 1,\n",
        "\n",
        " }\n",
        " # Define the DAG\n",
        "\n",
        " dag = DAG(\n",
        "\n",
        " ’etl_pipeline_mongodb’,\n",
        "\n",
        " default_args=default_args,\n",
        "\n",
        " schedule_interval=’0 6 * * *’, # Run every day at 6:00 AM\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "x7OeRISw7cmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Create the Sales CSV File**\n",
        "\n",
        " Create a file named sales.csv in a directory accessible by your Airflow DAG.\n",
        "\n",
        " It should contain\n",
        "\n",
        " sales data like the example below:\n",
        "\n",
        " transaction_id,customer_id,product_id,quantity,price\n",
        "\n",
        " T001,C001,P001,2,100\n",
        "\n",
        " T002,C002,P002,1,200\n",
        "\n",
        " T003,C003,P003,3,50"
      ],
      "metadata": {
        "id": "lvXncrxJ7tIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Step 5: Run the DAG**\n",
        " • Start the Airflow web server and navigate to http://localhost:8080.\n",
        " • Activate your DAG (etl pipeline mongodb) in the Airflow UI.\n",
        " • Manually trigger the DAG to test it. Verify the tasks in the DAG and check the logs\n",
        " to ensure successful execution."
      ],
      "metadata": {
        "id": "YGokZ1Ud74xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Schedule the DAG**\n",
        " By default, the DAG will be scheduled to run at 6:00 AM every day (schedule interval=’0 6* * *’). Verify the schedule by checking the DAG’s configuration in the Airflow UI."
      ],
      "metadata": {
        "id": "8NddGQd98HD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "\n",
        "\n",
        "# Define default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2025, 1, 1),\n",
        "    'retries': 2,\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'etl_pipeline_mongodb',\n",
        "    default_args=default_args,\n",
        "    schedule='0 6 * * *',  # Run every day at 6:00 AM\n",
        ")\n",
        "\n",
        "# Define the extract task\n",
        "def extract_data(**kwargs):\n",
        "    # Read the CSV file containing sales data\n",
        "    url = 'https://raw.githubusercontent.com/falawar7/AAI_634O/refs/heads/main/Week3/sales.csv'\n",
        "    df = pd.read_csv(url)\n",
        "    kwargs['ti'].xcom_push(key = 'extracted_df', value = df)\n",
        "    # Print the extracted data (this will be logged in the Airflow UI)\n",
        "    print(\"Extracted Data:\")\n",
        "    print(df)\n",
        "    return\n",
        "\n",
        "# Define the transform task\n",
        "def transform_data(**kwargs):\n",
        "    # Example transformation: calculate total sales for each product\n",
        "    df1 = kwargs['ti'].xcom_pull(key = 'extracted_df', task_ids = 'extract_data')\n",
        "    df1['total_sales'] = df1['quantity'] * df1['price']\n",
        "    # Print the transformed data (this will be logged in the Airflow UI)\n",
        "    kwargs['ti'].xcom_push(key = 'extracted_df_1', value = df1)\n",
        "    print(\"Transformed Data:\")\n",
        "    print(df1)\n",
        "    return\n",
        "\n",
        "# Define the load task\n",
        "def load_data(**kwargs):\n",
        "    # Load the transformed data into MongoDB\n",
        "    client = MongoClient(\"mongodb+srv://faysalelawar:pb6LB2kBPQ5Be5vN@dataengineeringcluster.61mrj.mongodb.net/?retryWrites=true&w=majority&appName=DataEngineeringCluster\")\n",
        "    db = client['sales_db_1']\n",
        "    collection = db['sales']\n",
        "    df = kwargs['ti'].xcom_pull(key = 'extracted_df_1', task_ids = 'transform_data')\n",
        "    # Convert the DataFrame to a dictionary of records\n",
        "    sales_data = df.to_dict(orient='records')\n",
        "    # Insert the data into MongoDB\n",
        "    collection.insert_many(sales_data)\n",
        "    # Print the loaded data (this will be logged in the Airflow UI)\n",
        "    print(\"Loaded Data into MongoDB:\")\n",
        "    for record in sales_data:\n",
        "        print(record)  # Print each record inserted into MongoDB\n",
        "\n",
        "# Define the tasks in the DAG\n",
        "extract_task = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "transform_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "load_task = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Set the task dependencies\n",
        "extract_task >> transform_task >> load_task"
      ],
      "metadata": {
        "id": "muH-VZ4B52XS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}